{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9qJP91vFiu09x4XoRJpKb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalit31/Data-Mining-Assignment/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLm7mnqoebYV",
        "outputId": "a7ce3290-f043-4899-e199-7de618666a19"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfJZm1RqZpQF",
        "outputId": "270d745f-702b-46f9-f555-7a659309d9a2"
      },
      "source": [
        "!pip install NRCLex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting NRCLex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/1c/0097ee39d456c8a92b2eb5dfd59f581a09a6bafede184a058fb0f19bb6ea/NRCLex-3.0.0.tar.gz (396kB)\n",
            "\r\u001b[K     |▉                               | 10kB 4.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 30kB 4.2MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 71kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 245kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 256kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 266kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 276kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 286kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 296kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 307kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 317kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 327kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 337kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 348kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 358kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 368kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 378kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 389kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 399kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (from NRCLex) (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob->NRCLex) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob->NRCLex) (1.15.0)\n",
            "Building wheels for collected packages: NRCLex\n",
            "  Building wheel for NRCLex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NRCLex: filename=NRCLex-3.0.0-cp37-none-any.whl size=43310 sha256=ce3853ea1fd7a564ff3dd6d14717f8379336601ea794e8cb6d687db9e377cb3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/31/64/035a8d245b4c217aeb8e8a2702d05dc91544b9c2334db72414\n",
            "Successfully built NRCLex\n",
            "Installing collected packages: NRCLex\n",
            "Successfully installed NRCLex-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaCu6YhXncDQ",
        "outputId": "38407f12-a2e4-40fd-a0cc-bb27db7faf01"
      },
      "source": [
        "!pip install sentistrength"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentistrength\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/cc/ffa07c024a6f90ad976a523fb696b689baab3459e1b8900f5553c42d3e19/sentistrength-0.0.9-py3-none-any.whl\n",
            "Installing collected packages: sentistrength\n",
            "Successfully installed sentistrength-0.0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KWXr-e6-2lF",
        "outputId": "6bdee9ee-0557-4999-878f-c1167ad45849"
      },
      "source": [
        "!bash   #1.python -m textblob.download_corpora  # 2. exit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bash: cannot set terminal process group (58): Inappropriate ioctl for device\n",
            "bash: no job control in this shell\n",
            "\u001b[01;34m/content\u001b[00m# python -m textblob.download_corpora\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n",
            "\u001b[01;34m/content\u001b[00m# exit\n",
            "exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xONjWwcmjoH"
      },
      "source": [
        "# import necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from sentistrength import PySentiStr\n",
        "from nrclex import NRCLex\n",
        "\n",
        "import io\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.linear_model import  LogisticRegression\n",
        "from sklearn import svm\n",
        "\n",
        "from scipy.sparse import coo_matrix, hstack\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRi128iAlxqE"
      },
      "source": [
        "senti = PySentiStr()\n",
        "senti.setSentiStrengthPath('/content/gdrive/My Drive/DM project/SentiStrengthCom.jar') # Note: Provide absolute path instead of relative path\n",
        "senti.setSentiStrengthLanguageFolderPath('/content/gdrive/My Drive/DM project/SentStrength_Data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PATbEFS4SPeW"
      },
      "source": [
        "# **Loading Dataset and Plots**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SVxFmWJWl-g"
      },
      "source": [
        "# load all three datasets\n",
        "trainFile = 'https://raw.githubusercontent.com/bansal01yash/DM-Proj/main/train2.tsv'\n",
        "testFile = 'https://raw.githubusercontent.com/bansal01yash/DM-Proj/main/test2.tsv'\n",
        "valFile = 'https://raw.githubusercontent.com/bansal01yash/DM-Proj/main/val2.tsv'\n",
        "\n",
        "# add header to all three datasets\n",
        "train_data = pd.read_csv(trainFile, delimiter='\\t',  names=[\"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Job Title\", \"State\", \"Party\",\n",
        "                         \"Barely True Cnt\", \"False Cnt\", \"Half True Cnt\", \"Mostly True Cnt\", \"Pants on Fire Cnt\", \"Context\", \"Justification\"])\n",
        "train_data.name = 'Training Data'\n",
        "\n",
        "test_data = pd.read_csv(testFile, delimiter='\\t',  names=[\"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Job Title\", \"State\", \"Party\",\n",
        "                         \"Barely True Cnt\", \"False Cnt\", \"Half True Cnt\", \"Mostly True Cnt\", \"Pants on Fire Cnt\", \"Context\", \"Justification\"])\n",
        "\n",
        "test_data.name = 'Testing Data'\n",
        "\n",
        "val_data = pd.read_csv(valFile, delimiter='\\t', names=[\"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Job Title\", \"State\", \"Party\",\n",
        "                         \"Barely True Cnt\", \"False Cnt\", \"Half True Cnt\", \"Mostly True Cnt\", \"Pants on Fire Cnt\", \"Context\", \"Justification\"])\n",
        "val_data.name = 'Validation Data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUScfZy4iS-_",
        "outputId": "ce0395e7-da81-458d-cd8e-1c5464e7465a"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(val_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10242, 15)\n",
            "(1267, 15)\n",
            "(1284, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OXh8uDycOu3"
      },
      "source": [
        "train_data[\"train-test-val\"] = 0\n",
        "val_data[\"train-test-val\"] = 1\n",
        "test_data[\"train-test-val\"] = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah-qXY3JbIMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c61b91-30a9-4918-f6b7-64e0fac8aec0"
      },
      "source": [
        "df = pd.concat([train_data,test_data,val_data]).reset_index(drop=True)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12793, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AThVjdI0mCCW"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le_multi = LabelEncoder()\n",
        "true_labels = ['half-true', 'mostly-true', 'true']\n",
        "text_features = ['Statement', 'Context', 'Subject', 'Job Title', 'Justification']\n",
        "sentiment=['fear','anger', 'anticip', 'trust','surprise', 'positive', 'negative', 'sadness', 'disgust', 'joy']\n",
        "\n",
        "def dataPreprocessing(df):\n",
        "\n",
        "    # Removing Appropriate Null Values\n",
        "    df = df[df['ID'].notna()]\n",
        "    # df = df[df['Barely True Cnt'].notna()]\n",
        "    # df = df[df['False Cnt'].notna()]\n",
        "    # df = df[df['Mostly True Cnt'].notna()]\n",
        "    # df = df[df['Pants on Fire Cnt'].notna()]\n",
        "    # df = df[df['Half True Cnt'].notna()]\n",
        "    df = df[df['Statement'].notna()]\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    #for Binary Classification\n",
        "    df['is_fake'] = df.Label.apply(lambda x: 1 if x in true_labels else 0)\n",
        "    df.loc[:, 'Label'] = le_multi.fit_transform(df.Label)\n",
        "\n",
        "    for feature in text_features:\n",
        "        #df = df[df[feature].notna()]\n",
        "        df[feature].fillna('None', inplace=True)\n",
        "        df[feature] = df[feature].str.replace(r'[^\\w\\s]+', ' ')       # Punctuation deleting\n",
        "        #df[feature] = df[feature].str.lower()                         # Lowercase \n",
        "        df[feature] = df[feature].str.replace('\\w*\\d+\\w*','NUM')      # Numbers replacement with \"NUM\" token\n",
        "        df[feature] = df[feature].str.replace('\\s{2,}',' ')           # Extra whitespaces removing\n",
        "\n",
        "    #text preprocessing for speaker\n",
        "      #Less common speakers replacement with token \"unknown_speaker\" (Less common definition by 11th percentile)\n",
        "      #One-hot-Encoding\n",
        "    df['Speaker'].fillna('unkown_speaker', inplace=True)\n",
        "    df.loc[~df.Speaker.isin(df.groupby('Speaker', as_index=False).count()[['Speaker', 'ID']].query(\"ID   > 11\").Speaker),'Speaker'] = 'unknown_speaker'\n",
        "    df = df.join(pd.get_dummies(df.Speaker, prefix='Speaker'))\n",
        "\n",
        "\n",
        "    \n",
        "    #text preprocessing for State\n",
        "      #Deleting some noise\n",
        "      #One-hot-Encoding\n",
        "    df['State'].fillna('noStateInfo', inplace=True)\n",
        "    df.loc[df.State.isin(['None', 'Unknown']), 'State'] = 'noStateInfo'\n",
        "    df.loc[df.State.isin(['Tennesse']), 'State'] = 'Tennessee'\n",
        "    df.loc[df.State.isin(['PA - Pennsylvania']), 'State'] = 'Pennsylvania'\n",
        "    df.loc[df.State.isin(['Rhode island']), 'State'] = 'Rhode Island'\n",
        "    df.loc[df.State.isin(['Tex']), 'State'] = 'Texas'\n",
        "    df.loc[df.State.isin(['Virgiia','Virgina', 'Virginia director, Coalition to Stop Gun Violence']), 'State'] = 'Virginia'\n",
        "    df.loc[df.State.isin(['Washington D.C.','Washington DC','Washington State', 'Washington, D.C.',]), 'State'] = 'Washington'\n",
        "\n",
        "    df = df.join(pd.get_dummies(df.State, prefix='State'))\n",
        "\n",
        "    #One-hot-Encoding for party\n",
        "    df['Party'].fillna('none', inplace=True)\n",
        "    df = df.join(pd.get_dummies(df.Party, prefix='Party'))\n",
        "\n",
        "    sdf=pd.DataFrame(senti.getSentiment(df['Statement'],score='trinary'),columns=['s_negative','s_neutral','s_positive'])\n",
        "    df.index=sdf.index\n",
        "    df=pd.concat([df,sdf],axis=1)\n",
        "\n",
        "    for y in sentiment:\n",
        "        df[y] = 0\n",
        "\n",
        "    for i in range (len(df)): \n",
        "        cur=(NRCLex(df['Statement'][i]).affect_frequencies)\n",
        "        for y in sentiment:\n",
        "            if y in cur:\n",
        "                df[y][i] = 1\n",
        "\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MECeWPPObh1s"
      },
      "source": [
        "df_pre=dataPreprocessing(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV8sSEC5e4d7",
        "outputId": "a165b525-4559-4fe0-8ea8-4778c884b3bb"
      },
      "source": [
        "traindata=df_pre[df_pre['train-test-val']==0]\n",
        "print(traindata.shape)\n",
        "valdata=df_pre[df_pre['train-test-val']==1]\n",
        "print(valdata.shape)\n",
        "testdata=df_pre[df_pre['train-test-val']==2]\n",
        "print(testdata.shape)\n",
        "{l: i for i, l in enumerate(le_multi.classes_)}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10240, 289)\n",
            "(1284, 289)\n",
            "(1267, 289)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'barely-true': 0,\n",
              " 'false': 1,\n",
              " 'half-true': 2,\n",
              " 'mostly-true': 3,\n",
              " 'pants-fire': 4,\n",
              " 'true': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE56t0WwktmZ"
      },
      "source": [
        "df_pre.to_csv('data_preprocessed.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}