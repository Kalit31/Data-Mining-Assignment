{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.corpus\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn import svm\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade gensim==3.8.3\n",
    "#!pip3 install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all three datasets\n",
    "trainFilePath = 'dataset/train2.tsv'\n",
    "testFilePath = 'dataset/test2.tsv'\n",
    "validationFilePath = 'dataset/val2.tsv'\n",
    "\n",
    "# add header to all three datasets\n",
    "df_train = pd.read_csv(trainFilePath, delimiter='\\t',  names=[\"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Job Title\", \"State\", \"Party\",\n",
    "                         \"Barely True Cnt\", \"False Cnt\", \"Half True Cnt\", \"Mostly True Cnt\", \"Pants on Fire Cnt\", \"Context\", \"Justification\"])\n",
    "\n",
    "df_test = pd.read_csv(testFilePath, delimiter='\\t',  names=[\"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Job Title\", \"State\", \"Party\",\n",
    "                         \"Barely True Cnt\", \"False Cnt\", \"Half True Cnt\", \"Mostly True Cnt\", \"Pants on Fire Cnt\", \"Context\", \"Justification\"])\n",
    "\n",
    "\n",
    "df_validation = pd.read_csv(validationFilePath, delimiter='\\t', names=[\"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Job Title\", \"State\", \"Party\",\n",
    "                         \"Barely True Cnt\", \"False Cnt\", \"Half True Cnt\", \"Mostly True Cnt\", \"Pants on Fire Cnt\", \"Context\", \"Justification\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"train-test-val\"] = 0\n",
    "df_test[\"train-test-val\"] = 1\n",
    "df_validation[\"train-test-val\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train,df_test,df_validation]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleaning(df,field):\n",
    "    df[field] = df[field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[field] = df[field].str.replace(r\"[^A-Za-z0-9]\", \" \")\n",
    "    df[field] = df[field].str.replace(r\"(),!?@\\'\\`\\\"\\_\\n\", \" \")\n",
    "    df[field] = df[field].str.replace(r\"@\", \"at\")\n",
    "    df[field] = df[field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[field] = df[field].str.replace(r\"http\", \"\")\n",
    "    df[field] = df[field].str.lower()\n",
    "    return df\n",
    "\n",
    "def dataPreprocessing(df):\n",
    "    df = df[df['ID'].notna()]\n",
    "    df = df[df['Barely True Cnt'].notna()]\n",
    "    df = df[df['False Cnt'].notna()]\n",
    "    df = df[df['Mostly True Cnt'].notna()]\n",
    "    df = df[df['Pants on Fire Cnt'].notna()]\n",
    "    df = df[df['Half True Cnt'].notna()]\n",
    "\n",
    "    df['ID'] = df['ID'].str.split(\".\", n = 1, expand = True) \n",
    "    \n",
    "    df = dataCleaning(df,'Statement')\n",
    "    df = dataCleaning(df,'Subject')\n",
    "    df = dataCleaning(df,'Speaker')\n",
    "    df = dataCleaning(df,'Job Title')\n",
    "    df = dataCleaning(df,'State')\n",
    "    df = dataCleaning(df,'Party')\n",
    "    df = dataCleaning(df,'Context')\n",
    "    df = dataCleaning(df,'Justification')    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = dataPreprocessing(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = df_all[['Label','train-test-val']]\n",
    "encoder= ce.OrdinalEncoder(cols=['Label'],return_df=True,\n",
    "                           mapping=[{'col':'Label',\n",
    "'mapping':{'pants-fire':0,'false':1,'mostly-false':2,'half-true':3,'mostly-true':4,'true':5}}])\n",
    "df_y = encoder.fit_transform(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[['Label','Statement','train-test-val']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom Statement to Unigram tokens\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"Unigrams\"] = df[\"Statement\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 13572\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary\n",
    "allUnigrams = []\n",
    "for unigrams in df['Unigrams']:\n",
    "    for unigram in unigrams:\n",
    "        allUnigrams.append(unigram)\n",
    "vocabulary = sorted(list(set(allUnigrams)))\n",
    "print(\"Vocabulary Size: \"+str(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT to download pretrained word2vec \n",
    "\n",
    "# import gensim.downloader as api\n",
    "# path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "# print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = '/home/kalit/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(unigrams, generate_missing=False, k=300):\n",
    "    if len(unigrams)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [model[word] if word in model else np.random.rand(k) for word in unigrams]\n",
    "    else:\n",
    "        vectorized = [model[word] if word in model else np.zeros(k) for word in unigrams]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(df, generate_missing=False):\n",
    "    embeddings = df['Unigrams'].apply(lambda x: get_word2vec(x,generate_missing=generate_missing))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_word2vec_embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedded_words = pd.DataFrame.from_records(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedded_words[\"train-test-val\"] = df[\"train-test-val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_embedded_words[df_embedded_words['train-test-val']==0]\n",
    "x_test = df_embedded_words[df_embedded_words['train-test-val']==1]\n",
    "x_val = df_embedded_words[df_embedded_words['train-test-val']==2]\n",
    "\n",
    "x_train.drop(['train-test-val'], axis = 1, inplace = True) \n",
    "x_test.drop(['train-test-val'], axis = 1, inplace = True)\n",
    "x_val.drop(['train-test-val'], axis = 1, inplace = True)\n",
    "\n",
    "y_train = df_y[df_y['train-test-val']==0]\n",
    "y_test = df_y[df_y['train-test-val']==1]\n",
    "y_val = df_y[df_y['train-test-val']==2]\n",
    "\n",
    "y_train.drop(['train-test-val'], axis = 1, inplace = True)\n",
    "y_test.drop(['train-test-val'], axis = 1, inplace = True)\n",
    "y_val.drop(['train-test-val'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', random_state=13)\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')    \n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.168, precision = 0.184, recall = 0.168, F1 = 0.172\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, F1 = get_metrics(y_test, y_predict)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, F1 = %.3f\" % (accuracy, precision,recall, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit0bc097d690354a299f7aa7a9b08339f8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
