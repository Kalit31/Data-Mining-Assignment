{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3WayBERTbinary.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyKkzqX6PXlg",
        "outputId": "72478682-c832-4419-aa0b-e4095c0f1f00"
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.17.61)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.4.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.61 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.20.61)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.61->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.61->boto3->pytorch-pretrained-bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yg5tDTU5Vlv"
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from random import randrange\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from matplotlib import pyplot as plt\n",
        "from argparse import ArgumentParser\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXB6lZhe5ZNX",
        "outputId": "f8e60a51-24e6-4c6c-e7c6-d363db1d0492"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "num_labels = 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN18o8I05z22"
      },
      "source": [
        "train_path = 'https://raw.githubusercontent.com/manideep2510/siamese-BERT-fake-news-detection-LIAR/master/LIAR-PLUS/dataset/train2.tsv'\n",
        "test_path = 'https://raw.githubusercontent.com/manideep2510/siamese-BERT-fake-news-detection-LIAR/master/LIAR-PLUS/dataset/test2.tsv'\n",
        "val_path = 'https://raw.githubusercontent.com/manideep2510/siamese-BERT-fake-news-detection-LIAR/master/LIAR-PLUS/dataset/val2.tsv'\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
        "test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
        "val_df = pd.read_csv(val_path, sep=\"\\t\", header=None)\n",
        "\n",
        "# Fill nan (empty boxes) with 0\n",
        "train_df = train_df.fillna(0)\n",
        "test_df = test_df.fillna(0)\n",
        "val_df = val_df.fillna(0)\n",
        "\n",
        "train = train_df.values\n",
        "test = test_df.values\n",
        "val = val_df.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFQDFYil58yp"
      },
      "source": [
        "labels = {'train':[train[i][2] for i in range(len(train))], 'test':[test[i][2] for i in range(len(test))], 'val':[val[i][2] for i in range(len(val))]}\n",
        "statements = {'train':[train[i][3] for i in range(len(train))], 'test':[test[i][3] for i in range(len(test))], 'val':[val[i][3] for i in range(len(val))]}\n",
        "subjects = {'train':[train[i][4] for i in range(len(train))], 'test':[test[i][4] for i in range(len(test))], 'val':[val[i][4] for i in range(len(val))]}\n",
        "speakers = {'train':[train[i][5] for i in range(len(train))], 'test':[test[i][5] for i in range(len(test))], 'val':[val[i][5] for i in range(len(val))]}\n",
        "jobs = {'train':[train[i][6] for i in range(len(train))], 'test':[test[i][6] for i in range(len(test))], 'val':[val[i][6] for i in range(len(val))]}\n",
        "states = {'train':[train[i][7] for i in range(len(train))], 'test':[test[i][7] for i in range(len(test))], 'val':[val[i][7] for i in range(len(val))]}\n",
        "affiliations = {'train':[train[i][8] for i in range(len(train))], 'test':[test[i][8] for i in range(len(test))], 'val':[val[i][8] for i in range(len(val))]}\n",
        "credits = {'train':[train[i][9:14] for i in range(len(train))], 'test':[test[i][9:14] for i in range(len(test))], 'val':[val[i][9:14] for i in range(len(val))]}\n",
        "contexts = {'train':[train[i][14] for i in range(len(train))], 'test':[test[i][14] for i in range(len(test))], 'val':[val[i][14] for i in range(len(val))]}\n",
        "justification = {'train':[train[i][15] for i in range(len(train))], 'test':[test[i][15] for i in range(len(test))], 'val':[val[i][15] for i in range(len(val))]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hWSLFe66DPW"
      },
      "source": [
        "if num_labels == 6:\n",
        "\n",
        "    def to_onehot(a):\n",
        "        a_cat = [0]*len(a)\n",
        "        for i in range(len(a)):\n",
        "            if a[i]=='true':\n",
        "                a_cat[i] = [1,0,0,0,0,0]\n",
        "            elif a[i]=='mostly-true':\n",
        "                a_cat[i] = [0,1,0,0,0,0]\n",
        "            elif a[i]=='half-true':\n",
        "                a_cat[i] = [0,0,1,0,0,0]\n",
        "            elif a[i]=='barely-true':\n",
        "                a_cat[i] = [0,0,0,1,0,0]\n",
        "            elif a[i]=='false':\n",
        "                a_cat[i] = [0,0,0,0,1,0]\n",
        "            elif a[i]=='pants-fire':\n",
        "                a_cat[i] = [0,0,0,0,0,1]\n",
        "            else:\n",
        "                print('Incorrect label')\n",
        "        return a_cat\n",
        "\n",
        "elif num_labels == 2:\n",
        "\n",
        "    def to_onehot(a):\n",
        "        a_cat = [0]*len(a)\n",
        "        for i in range(len(a)):\n",
        "            if a[i]=='true':\n",
        "                a_cat[i] = [1,0]\n",
        "            elif a[i]=='mostly-true':\n",
        "                a_cat[i] = [1,0]\n",
        "            elif a[i]=='half-true':\n",
        "                a_cat[i] = [1,0]\n",
        "            elif a[i]=='barely-true':\n",
        "                a_cat[i] = [0,1]\n",
        "            elif a[i]=='false':\n",
        "                a_cat[i] = [0,1]\n",
        "            elif a[i]=='pants-fire':\n",
        "                a_cat[i] = [0,1]\n",
        "            else:\n",
        "                print('Incorrect label')\n",
        "        return a_cat\n",
        "\n",
        "else:\n",
        "\n",
        "    print('Invalid number of labels. The number of labels should be either 2 or 6')\n",
        "\n",
        "    sys.exit()\n",
        "\n",
        "\n",
        "labels_onehot = {'train':to_onehot(labels['train']), 'test':to_onehot(labels['test']), 'val':to_onehot(labels['val'])}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrXxwaZ16ELO"
      },
      "source": [
        "metadata = {'train':[0]*len(train), 'val':[0]*len(val), 'test':[0]*len(test)}\n",
        "\n",
        "for i in range(len(train)):\n",
        "    subject = subjects['train'][i]\n",
        "    if subject == 0:\n",
        "        subject = 'None'\n",
        "\n",
        "    speaker = speakers['train'][i]\n",
        "    if speaker == 0:\n",
        "        speaker = 'None'\n",
        "\n",
        "    job = jobs['train'][i]\n",
        "    if job == 0:\n",
        "        job = 'None'\n",
        "\n",
        "    state = states['train'][i]\n",
        "    if state == 0:\n",
        "        state = 'None'\n",
        "\n",
        "    affiliation = affiliations['train'][i]\n",
        "    if affiliation == 0:\n",
        "        affiliation = 'None'\n",
        "\n",
        "    context = contexts['train'][i]\n",
        "    if context == 0 :\n",
        "        context = 'None'\n",
        "\n",
        "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
        "\n",
        "    metadata['train'][i] = meta\n",
        "\n",
        "for i in range(len(val)):\n",
        "    subject = subjects['val'][i]\n",
        "    if subject == 0:\n",
        "        subject = 'None'\n",
        "\n",
        "    speaker = speakers['val'][i]\n",
        "    if speaker == 0:\n",
        "        speaker = 'None'\n",
        "\n",
        "    job = jobs['val'][i]\n",
        "    if job == 0:\n",
        "        job = 'None'\n",
        "\n",
        "    state = states['val'][i]\n",
        "    if state == 0:\n",
        "        state = 'None'\n",
        "\n",
        "    affiliation = affiliations['val'][i]\n",
        "    if affiliation == 0:\n",
        "        affiliation = 'None'\n",
        "\n",
        "    context = contexts['val'][i]\n",
        "    if context == 0 :\n",
        "        context = 'None'\n",
        "\n",
        "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
        "\n",
        "    metadata['val'][i] = meta\n",
        "\n",
        "for i in range(len(test)):\n",
        "    subject = subjects['test'][i]\n",
        "    if subject == 0:\n",
        "        subject = 'None'\n",
        "\n",
        "    speaker = speakers['test'][i]\n",
        "    if speaker == 0:\n",
        "        speaker = 'None'\n",
        "\n",
        "    job = jobs['test'][i]\n",
        "    if job == 0:\n",
        "        job = 'None'\n",
        "\n",
        "    state = states['test'][i]\n",
        "    if state == 0:\n",
        "        state = 'None'\n",
        "\n",
        "    affiliation = affiliations['test'][i]\n",
        "    if affiliation == 0:\n",
        "        affiliation = 'None'\n",
        "\n",
        "    context = contexts['test'][i]\n",
        "    if context == 0 :\n",
        "        context = 'None'\n",
        "\n",
        "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
        "\n",
        "    metadata['test'][i] = meta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY6GWOAS6QnW"
      },
      "source": [
        "credit_score = {'train':[0]*len(train), 'val':[0]*len(val), 'test':[0]*len(test)}\n",
        "for i in range(len(train)):\n",
        "    credit = credits['train'][i]\n",
        "    if sum(credit) == 0:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
        "    credit_score['train'][i] = [score for i in range(2304)]\n",
        "\n",
        "for i in range(len(val)):\n",
        "    credit = credits['val'][i]\n",
        "    if sum(credit) == 0:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
        "    credit_score['val'][i] = [score for i in range(2304)]\n",
        "\n",
        "for i in range(len(test)):\n",
        "    credit = credits['test'][i]\n",
        "    if sum(credit) == 0:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
        "    credit_score['test'][i] = [score for i in range(2304)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrDB-lJH6aut"
      },
      "source": [
        "class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRIiVILK677N"
      },
      "source": [
        "class BertForSequenceClassification(nn.Module):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, ..., num_labels].\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits of shape [batch_size, num_labels].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    num_labels = 2\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, num_labels=2): # Change number of labels here.\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size*3, num_labels)\n",
        "        #self.fc1 = nn.Linear(config.hidden_size*2, 512)\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "\n",
        "    '''def forward_once(self, x):\n",
        "        # Forward pass\n",
        "        output = self.cnn1(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc1(output)\n",
        "        return output'''\n",
        "\n",
        "    def forward_once(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        #logits = self.classifier(pooled_output)\n",
        "\n",
        "        return pooled_output\n",
        "\n",
        "    def forward(self, input_ids1, input_ids2, input_ids3, credit_sc):\n",
        "        # forward pass of input 1\n",
        "        output1 = self.forward_once(input_ids1, token_type_ids=None, attention_mask=None, labels=None)\n",
        "        # forward pass of input 2\n",
        "        output2 = self.forward_once(input_ids2, token_type_ids=None, attention_mask=None, labels=None)\n",
        "\n",
        "        output3 = self.forward_once(input_ids3, token_type_ids=None, attention_mask=None, labels=None)\n",
        "\n",
        "        out = torch.cat((output1, output2, output3), 1)\n",
        "        #print(out.shape)\n",
        "\n",
        "        # Multiply the credit score with the output after concatnation\n",
        "\n",
        "        out = torch.add(credit_sc, out)\n",
        "\n",
        "        #out = self.fc1(out)\n",
        "        logits = self.classifier(out)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def freeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8efIZNGt69MW",
        "outputId": "f7f19206-dd1f-441e-dda7-1ec18dd23481"
      },
      "source": [
        "from pytorch_pretrained_bert import BertConfig\n",
        "\n",
        "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification(num_labels)\n",
        "model.freeze_bert_encoder()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpglz5ylsk\n",
            "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWcu8N9JNVRZ"
      },
      "source": [
        "# Loading the statements\n",
        "X_train = statements['train']\n",
        "y_train = labels_onehot['train']\n",
        "\n",
        "X_val = statements['val']\n",
        "y_val = labels_onehot['val']\n",
        "\n",
        "X_train = X_train + X_val\n",
        "y_train = y_train + y_val\n",
        "\n",
        "\n",
        "X_test = statements['test']\n",
        "y_test = labels_onehot['test']\n",
        "\n",
        "# Loading the justification\n",
        "X_train_just = justification['train']\n",
        "\n",
        "X_val_just = justification['val']\n",
        "\n",
        "X_train_just = X_train_just + X_val_just\n",
        "\n",
        "X_test_just = statements['test']\n",
        "\n",
        "\n",
        "# Loading the meta data\n",
        "X_train_meta = metadata['train']\n",
        "X_val_meta = metadata['val']\n",
        "X_train_meta = X_train_meta + X_val_meta\n",
        "X_test_meta = metadata['test']\n",
        "\n",
        "# Loading Credit scores\n",
        "\n",
        "X_train_credit = credit_score['train']\n",
        "X_val_credit = credit_score['val']\n",
        "X_train_credit = X_train_credit+X_val_credit\n",
        "X_test_credit = credit_score['test']\n",
        "\n",
        "\n",
        "# Small data partitioned for debugging\n",
        "'''X_train = X_train[:100]\n",
        "y_train = y_train[:100]\n",
        "X_test = X_test[:100]\n",
        "y_test = y_test[:100]\n",
        "X_train_just = X_train_just[:100]\n",
        "X_test_just = X_test_just[:100]\n",
        "X_train_meta = X_train_meta[:100]\n",
        "X_test_meta = X_test_meta[:100]\n",
        "X_train_credit = X_train_credit[:100]\n",
        "X_test_credit = X_test_credit[:100]'''\n",
        "\n",
        "max_seq_length_stat = 64\n",
        "max_seq_length_just = 256\n",
        "max_seq_length_meta = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ik80mg3Nf0t"
      },
      "source": [
        "class text_dataset(Dataset):\n",
        "    def __init__(self,x_y_list, transform=None):\n",
        "\n",
        "        self.x_y_list = x_y_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "\n",
        "        # Tokenize statements\n",
        "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
        "\n",
        "        if len(tokenized_review) > max_seq_length_stat:\n",
        "            tokenized_review = tokenized_review[:max_seq_length_stat]\n",
        "\n",
        "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
        "\n",
        "        padding = [0] * (max_seq_length_stat - len(ids_review))\n",
        "\n",
        "        ids_review += padding\n",
        "\n",
        "        assert len(ids_review) == max_seq_length_stat\n",
        "\n",
        "        #print(ids_review)\n",
        "        ids_review = torch.tensor(ids_review)\n",
        "\n",
        "        fakeness = self.x_y_list[4][index] # color\n",
        "        list_of_labels = [torch.from_numpy(np.array(fakeness))]\n",
        "\n",
        "\n",
        "        # Tokenize justifications\n",
        "        #print(self.x_y_list[1][6833])\n",
        "        #print(index)\n",
        "\n",
        "        # Making sure that if there is no justification in a row(nan value converted to 0 using pandas), give it a justification called 'No justification' for training to be possible.\n",
        "        if self.x_y_list[1][index] == 0:\n",
        "            self.x_y_list[1][index] = 'No justification'\n",
        "\n",
        "        tokenized_review_just = tokenizer.tokenize(self.x_y_list[1][index])\n",
        "\n",
        "        if len(tokenized_review_just) > max_seq_length_just:\n",
        "            tokenized_review_just = tokenized_review_just[:max_seq_length_just]\n",
        "\n",
        "        ids_review_just  = tokenizer.convert_tokens_to_ids(tokenized_review_just)\n",
        "\n",
        "        padding = [0] * (max_seq_length_just - len(ids_review_just))\n",
        "\n",
        "        ids_review_just += padding\n",
        "\n",
        "        assert len(ids_review_just) == max_seq_length_just\n",
        "\n",
        "        #print(ids_review)\n",
        "        ids_review_just = torch.tensor(ids_review_just)\n",
        "\n",
        "        fakeness = self.x_y_list[4][index] # color\n",
        "        list_of_labels = [torch.from_numpy(np.array(fakeness))]\n",
        "\n",
        "        # Tokenize metadata\n",
        "\n",
        "        tokenized_review_meta = tokenizer.tokenize(self.x_y_list[2][index])\n",
        "\n",
        "        if len(tokenized_review_meta) > max_seq_length_meta:\n",
        "            tokenized_review_meta = tokenized_review_meta[:max_seq_length_meta]\n",
        "\n",
        "        ids_review_meta  = tokenizer.convert_tokens_to_ids(tokenized_review_meta)\n",
        "\n",
        "        padding = [0] * (max_seq_length_meta - len(ids_review_meta))\n",
        "\n",
        "        ids_review_meta += padding\n",
        "\n",
        "        assert len(ids_review_meta) == max_seq_length_meta\n",
        "\n",
        "        #print(ids_review)\n",
        "        ids_review_meta = torch.tensor(ids_review_meta)\n",
        "\n",
        "        fakeness = self.x_y_list[4][index] # color\n",
        "        list_of_labels = [torch.from_numpy(np.array(fakeness))]\n",
        "\n",
        "        credit_scr = self.x_y_list[3][index] # Credit score\n",
        "\n",
        "        #ones_768 = np.ones((768))\n",
        "        #credit_scr = credit_scr * ones_768\n",
        "        credit_scr = torch.tensor(credit_scr)\n",
        "\n",
        "        return [ids_review, ids_review_just, ids_review_meta, credit_scr], list_of_labels[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_y_list[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNjkU6mkOFID"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "# Train Statements and Justifications\n",
        "train_lists = [X_train, X_train_just, X_train_meta, X_train_credit, y_train]\n",
        "\n",
        "# Test Statements and Justifications\n",
        "test_lists = [X_test, X_test_just, X_train_meta, X_test_credit, y_test]\n",
        "\n",
        "# Preparing the data (Tokenize)\n",
        "training_dataset = text_dataset(x_y_list = train_lists)\n",
        "test_dataset = text_dataset(x_y_list = test_lists)\n",
        "\n",
        "\n",
        "# Prepare the training dictionaries\n",
        "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "                   'val':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "                   }\n",
        "dataset_sizes = {'train':len(train_lists[0]),\n",
        "                'val':len(test_lists[0])}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y5wvMfXOHIL",
        "outputId": "c96d23f7-c3ad-44a1-b4d0-127d07bfc744"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iazzwi8lOKPQ"
      },
      "source": [
        "train_acc = []\n",
        "val_acc = []\n",
        "train_loss = []\n",
        "val_loss = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z04PP0j8OUD0"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "    print('starting')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            fakeness_corrects = 0\n",
        "\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, fakeness in dataloaders_dict[phase]:\n",
        "\n",
        "                inputs1 = inputs[0] # News statement input\n",
        "                inputs2 = inputs[1] # Justification input\n",
        "                inputs3 = inputs[2] # Meta data input\n",
        "                inputs4 = inputs[3] # Credit scores input\n",
        "\n",
        "                inputs1 = inputs1.to(device)\n",
        "                inputs2 = inputs2.to(device)\n",
        "                inputs3 = inputs3.to(device)\n",
        "                inputs4 = inputs4.to(device)\n",
        "\n",
        "                fakeness = fakeness.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    #print(inputs)\n",
        "                    outputs = model(inputs1, inputs2, inputs3, inputs4)\n",
        "\n",
        "                    outputs = F.softmax(outputs,dim=1)\n",
        "\n",
        "                    loss = criterion(outputs, torch.max(fakeness.float(), 1)[1])\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs1.size(0)\n",
        "\n",
        "\n",
        "                fakeness_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(fakeness, 1)[1])\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "\n",
        "            fakeness_acc = fakeness_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
        "            print('{} fakeness_acc: {:.4f}'.format(\n",
        "                phase, fakeness_acc))\n",
        "\n",
        "            # Saving training acc and loss for each epoch\n",
        "            fakeness_acc1 = fakeness_acc.data\n",
        "            fakeness_acc1 = fakeness_acc1.cpu()\n",
        "            fakeness_acc1 = fakeness_acc1.numpy()\n",
        "            train_acc.append(fakeness_acc1)\n",
        "\n",
        "            #epoch_loss1 = epoch_loss.data\n",
        "            #epoch_loss1 = epoch_loss1.cpu()\n",
        "            #epoch_loss1 = epoch_loss1.numpy()\n",
        "            train_loss.append(epoch_loss)\n",
        "\n",
        "            if phase == 'val' and fakeness_acc > best_acc:\n",
        "                print('Saving with accuracy of {}'.format(fakeness_acc),\n",
        "                      'improved over previous {}'.format(best_acc))\n",
        "                best_acc = fakeness_acc\n",
        "\n",
        "                # Saving val acc and loss for each epoch\n",
        "                fakeness_acc1 = fakeness_acc.data\n",
        "                fakeness_acc1 = fakeness_acc1.cpu()\n",
        "                fakeness_acc1 = fakeness_acc1.numpy()\n",
        "                val_acc.append(fakeness_acc1)\n",
        "\n",
        "                #epoch_loss1 = epoch_loss.data\n",
        "                #epoch_loss1 = epoch_loss1.cpu()\n",
        "                #epoch_loss1 = epoch_loss1.numpy()\n",
        "                val_loss.append(epoch_loss)\n",
        "\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), 'bert_model_test_noFC1_triBERT_binary_focalloss.pth')\n",
        "\n",
        "        print('Time taken for epoch'+ str(epoch+1)+ ' is ' + str((time.time() - epoch_start)/60) + ' minutes')\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(float(best_acc)))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_acc, val_acc, train_loss, val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWO6QWipOdGQ"
      },
      "source": [
        "model.to(device)\n",
        "lrlast = .0001\n",
        "lrmain = .00001\n",
        "optim1 = optim.Adam(\n",
        "    [\n",
        "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
        "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
        "\n",
        "   ])\n",
        "\n",
        "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "'''import focal_loss\n",
        "loss_args = {\"alpha\": 0.5, \"gamma\": 2.0}\n",
        "criterion = focal_loss.FocalLoss(*loss_args)'''\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 3 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV9V-xWROiRl",
        "outputId": "9d04adb4-f3d5-4fd6-f5bc-a385c59a4809"
      },
      "source": [
        "model_ft1, train_acc, val_acc, train_loss, val_loss = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting\n",
            "Epoch 1/20\n",
            "----------\n",
            "train total loss: 0.5949 \n",
            "train fakeness_acc: 0.7004\n",
            "val total loss: 0.5669 \n",
            "val fakeness_acc: 0.7474\n",
            "Saving with accuracy of 0.7474348855564326 improved over previous 0\n",
            "Time taken for epoch1 is 5.536893701553344 minutes\n",
            "\n",
            "Epoch 2/20\n",
            "----------\n",
            "train total loss: 0.5755 \n",
            "train fakeness_acc: 0.7182\n",
            "val total loss: 0.5599 \n",
            "val fakeness_acc: 0.7435\n",
            "Time taken for epoch2 is 5.818092238903046 minutes\n",
            "\n",
            "Epoch 3/20\n",
            "----------\n",
            "train total loss: 0.5717 \n",
            "train fakeness_acc: 0.7224\n",
            "val total loss: 0.5585 \n",
            "val fakeness_acc: 0.7466\n",
            "Time taken for epoch3 is 5.835004043579102 minutes\n",
            "\n",
            "Epoch 4/20\n",
            "----------\n",
            "train total loss: 0.5677 \n",
            "train fakeness_acc: 0.7262\n",
            "val total loss: 0.5586 \n",
            "val fakeness_acc: 0.7419\n",
            "Time taken for epoch4 is 5.84060259660085 minutes\n",
            "\n",
            "Epoch 5/20\n",
            "----------\n",
            "train total loss: 0.5687 \n",
            "train fakeness_acc: 0.7238\n",
            "val total loss: 0.5584 \n",
            "val fakeness_acc: 0.7459\n",
            "Time taken for epoch5 is 5.832749756177266 minutes\n",
            "\n",
            "Epoch 6/20\n",
            "----------\n",
            "train total loss: 0.5684 \n",
            "train fakeness_acc: 0.7254\n",
            "val total loss: 0.5581 \n",
            "val fakeness_acc: 0.7451\n",
            "Time taken for epoch6 is 5.832080829143524 minutes\n",
            "\n",
            "Epoch 7/20\n",
            "----------\n",
            "train total loss: 0.5684 \n",
            "train fakeness_acc: 0.7246\n",
            "val total loss: 0.5581 \n",
            "val fakeness_acc: 0.7466\n",
            "Time taken for epoch7 is 5.839108395576477 minutes\n",
            "\n",
            "Epoch 8/20\n",
            "----------\n",
            "train total loss: 0.5675 \n",
            "train fakeness_acc: 0.7245\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7419\n",
            "Time taken for epoch8 is 5.836408408482869 minutes\n",
            "\n",
            "Epoch 9/20\n",
            "----------\n",
            "train total loss: 0.5681 \n",
            "train fakeness_acc: 0.7234\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7411\n",
            "Time taken for epoch9 is 5.834044353167216 minutes\n",
            "\n",
            "Epoch 10/20\n",
            "----------\n",
            "train total loss: 0.5678 \n",
            "train fakeness_acc: 0.7258\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7411\n",
            "Time taken for epoch10 is 5.846732906500498 minutes\n",
            "\n",
            "Epoch 11/20\n",
            "----------\n",
            "train total loss: 0.5688 \n",
            "train fakeness_acc: 0.7247\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7411\n",
            "Time taken for epoch11 is 5.84200979868571 minutes\n",
            "\n",
            "Epoch 12/20\n",
            "----------\n",
            "train total loss: 0.5689 \n",
            "train fakeness_acc: 0.7254\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7395\n",
            "Time taken for epoch12 is 5.8391565958658855 minutes\n",
            "\n",
            "Epoch 13/20\n",
            "----------\n",
            "train total loss: 0.5691 \n",
            "train fakeness_acc: 0.7254\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7395\n",
            "Time taken for epoch13 is 5.842874272664388 minutes\n",
            "\n",
            "Epoch 14/20\n",
            "----------\n",
            "train total loss: 0.5692 \n",
            "train fakeness_acc: 0.7224\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7395\n",
            "Time taken for epoch14 is 5.847642393906911 minutes\n",
            "\n",
            "Epoch 15/20\n",
            "----------\n",
            "train total loss: 0.5693 \n",
            "train fakeness_acc: 0.7234\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7395\n",
            "Time taken for epoch15 is 5.831179145971934 minutes\n",
            "\n",
            "Epoch 16/20\n",
            "----------\n",
            "train total loss: 0.5677 \n",
            "train fakeness_acc: 0.7267\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7395\n",
            "Time taken for epoch16 is 5.841610717773437 minutes\n",
            "\n",
            "Epoch 17/20\n",
            "----------\n",
            "train total loss: 0.5686 \n",
            "train fakeness_acc: 0.7257\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7395\n",
            "Time taken for epoch17 is 5.844596803188324 minutes\n",
            "\n",
            "Epoch 18/20\n",
            "----------\n",
            "train total loss: 0.5679 \n",
            "train fakeness_acc: 0.7220\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7395\n",
            "Time taken for epoch18 is 5.834027397632599 minutes\n",
            "\n",
            "Epoch 19/20\n",
            "----------\n",
            "train total loss: 0.5688 \n",
            "train fakeness_acc: 0.7222\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7395\n",
            "Time taken for epoch19 is 5.844284602006277 minutes\n",
            "\n",
            "Epoch 20/20\n",
            "----------\n",
            "train total loss: 0.5685 \n",
            "train fakeness_acc: 0.7254\n",
            "val total loss: 0.5583 \n",
            "val fakeness_acc: 0.7395\n",
            "Time taken for epoch20 is 5.844313013553619 minutes\n",
            "\n",
            "Training complete in 116m 28s\n",
            "Best val Acc: 0.747435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1YYix-COqin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb3f66b3-33fd-4f92-828f-5ef8e312f6ae"
      },
      "source": [
        "'''# Accuracy plots\n",
        "\n",
        "print(val_acc)\n",
        "print(val_loss)\n",
        "#plt.plot(train_acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['val'], loc='upper left')\n",
        "#plt.show()\n",
        "plt.savefig('accuracy.png')\n",
        "plt.close()\n",
        "print('Saved Accuracy plot')\n",
        "# Loss plots\n",
        "#plt.plot(train_loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['val'], loc='upper right')\n",
        "#plt.show()\n",
        "plt.savefig('loss.png')\n",
        "plt.close()\n",
        "print('Saved Loss plot')'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# Accuracy plots\\n\\nprint(val_acc)\\nprint(val_loss)\\n#plt.plot(train_acc)\\nplt.plot(val_acc)\\nplt.title('Model accuracy')\\nplt.ylabel('accuracy')\\nplt.xlabel('epoch')\\nplt.legend(['val'], loc='upper left')\\n#plt.show()\\nplt.savefig('accuracy.png')\\nplt.close()\\nprint('Saved Accuracy plot')\\n# Loss plots\\n#plt.plot(train_loss)\\nplt.plot(val_loss)\\nplt.title('Model loss')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\nplt.legend(['val'], loc='upper right')\\n#plt.show()\\nplt.savefig('loss.png')\\nplt.close()\\nprint('Saved Loss plot')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}